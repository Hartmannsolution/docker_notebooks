{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90304538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification\n",
    "## Part 1:\n",
    "\n",
    "### Use K-Nearest Neighbour on AirBnb [data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?resource=download)\n",
    "### [Reference to example code](https://www.tutorialspoint.com/scikit_learn/scikit_learn_kneighbors_classifier.htm)\n",
    "- The data file is already downloaded to: data/AB_NYC_2019.csv. Load it into pandas dataframe\n",
    "- Purpose of this exercise is to use K-Neares-Neighbor algorithm to make a binary classification in order to estimate if the price of a specific Airbnb accommodation will be above or below the median, \n",
    "- First we will try to do it based on **only 2 features: longitude and latitude.** \n",
    "- Next we will see if we can improve accuracy with using more features\n",
    "- As independent variables, we have location, neighborhood and the number of reviews the acommodation has on Airbnb.\n",
    "#### Part 1.1: 20 min.\n",
    "1. Use the following imports:\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix\n",
    "```\n",
    "2. Get the data into a pandas dataframe\n",
    "3. Add a column to the dataframe: \"is_cheap\", that contains boolean values for the price being below median. Hint: DataFrame has a median() method. This column contains our target data: y\n",
    "#### Part 1.2: 25 min\n",
    "4. Create a Classifier model with `KNeighborsClassifier()` and give it an arbitrary number for the n_neighbors argument\n",
    "5. Create input data: X as a DataFrame containing only longitude and latitude.\n",
    "5. Based on X and y (the target data) above, split data into training and test data using train_test_split() method with 33% test data.\n",
    "6. Fit the model with the training data. Hint: `knn_class.fit(X_train, y_train)`\n",
    "7. And make predictions with the test data. Hint: `knn_class.predict(X_test)`\n",
    "#### Part 1.3: 25 min\n",
    "8. Now we have our target and our predictions and we need to compare them to see how well our model have done. For this we ca use the roc_curve method like this: `fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=True)` where pos_label lets the algorithm know that our data uses boolean in the target column. This gives us the True Positive Rate (TPR) and the False Positive Rate (FPR). ROC Curve works by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. Finally we use the `auc(fpr,tpr)` function to get an AUC_Score (This score is 1 when the model had 100% correct predictions and less than 1 for less perfect accuracy score. The result should be around `.7` which is not a great prediction accuracy rate, but its a start and we can try to improve it by adding more data features to the model.\n",
    "Study: [ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py).\n",
    "9. Now lets add some more columns from the dataframe: \n",
    "    1. First we need to One-Hot encode the data of 3 columns:['neighbourhood','neighbourhood_group','room_type']. Hint: Use pandas get_dummies method (see example in the clustering with titanic notebook.\n",
    "    2. With these new columns in the dataframe do the train_test_split operation again to get 33% test data and 67% training data for both input data X and target/labels y.\n",
    "    3. Normalize both training and test data with [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Hint: `StandardScaler().fit(X_train[independent_variables])` where `independent_variables` is a list of all the columns we want to use in the model (There are many so a quick way to get the names of those columns that we One-Hot encoded is by using a list comprehension like this: `[col for col in df if col.startswith('neighbourhood') or col.startswith('room_type')]`. Then just add the 'latitude', 'longitude','number_of_reviews' and 'reviews_per_month' columns.\n",
    "    4. Now get the normalized training data with something like: `X_train_norm = np.nan_to_num(scaler.transform(X_train[independent_variables]))` where np.nan_to_num() is used to swap NAN for zeros.\n",
    "    5. Do the same with the test data\n",
    "    6. Now create a `KNeighborsClassifier` model like last time and fit it with the training data and the training targets\n",
    "    7. Get predictions on the test data and produce the AUC score like last time. Is it improved?\n",
    "    8. When we create our KneighborsClassifier model we can try it out with different number of neighbors and with different ways to measure the distance between the neighbors like this `KNeighborsClassifier(n_neighbors=k, metric=dist)`. [These are the different available methods for measuring distance.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.distance_metrics.html#sklearn.metrics.pairwise.distance_metrics). Now create a function that can take k and dist (as shown above) and can print an AUC score based on the data we used above and on the 2 arguments.\n",
    "    9. Run the function with all combinations of n_neighbor values of 2, 4, 8, 32, 64 and with metric values of 'manhattan', 'euclidean', 'haversine','cosine'.\n",
    "    10. Are there any noticable differences?\n",
    "    \n",
    "## Part 2 Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8dce3c-76be-41e2-8614-03701e3e5e4d",
   "metadata": {},
   "source": [
    "## Tutorial on [Cifar10](https://www.kaggle.com/code/roblexnana/cifar10-with-cnn-for-beginer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7569a05-8738-4daf-92e3-7083352e231b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
